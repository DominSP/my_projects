{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ocena klasyfikacji binarnej:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy** (Dokładność) to najprostsza miara jakości modelu, która mierzy, jaki procent wszystkich przewidywań był poprawny. Jest to **stosunek prawidłowo sklasyfikowanych** przykładów **do wszystkich** przykładów w zbiorze testowym.\n",
    "\n",
    "Najlepiej aby klasy były zrównoważone, inaczej accuracy może być nieodpowiednią miarą. Przykładowo, jeśli w jednej klasie jest 90 % przypadków, to accuracy może być wysokie, pomimo tego, że model źle klasyfikuje drugą klasę. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(correct, predicted):\n",
    "    return (correct == predicted).sum() / correct.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Precision** (Precyzja) mierzy, jak dokładnie model klasyfikuje pozytywne przypadki, czyli jaka część przykładów przewidzianych jako pozytywne (1) faktycznie była pozytywna. Jest to **stosunek poprawnie przewidzianych pozytywnych** etykiet **do wszystkich pozytywnie przewidzianych**, w tym niepoprawnie.\n",
    "\n",
    "Może się okazać ważna przykładowo w przypadku klasyfikacji chorób, wtedy chcemy żeby ta precyzja była jak największa. Model przewidywałby poprawnie chorobę z prawdopodobieństwem równym miary precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(correct, predicted):\n",
    "    tp = np.sum((correct == 1) & (predicted == 1))\n",
    "    fp = np.sum((correct == 0) & (predicted == 1))\n",
    "    return tp / (tp + fp) if (tp + fp) != 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recall** (Czułość, Wykrywalność) mierzy, jak wiele rzeczywistych pozytywnych przypadków model poprawnie wykrył. Jest to **stosunek liczby prawdziwych pozytywów do liczby wszystkich rzeczywistych pozytywnych** przykładów.\n",
    "\n",
    "W przypadkach, gdy pominięcie pozytywnych przypadków jest kosztowne (np. wykrywanie oszustw – lepiej wykryć więcej oszustw, nawet kosztem fałszywych alarmów)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(correct, predicted):\n",
    "    tp = np.sum((correct == 1) & (predicted == 1))\n",
    "    fn = np.sum((correct == 1) & (predicted == 0))\n",
    "    return tp / (tp + fn) if (tp + fn) != 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Niech przykładowy model klasyfikuje e-maile jako spam (klasa 1) lub nie spam (klasa 0).\n",
    "\n",
    "-> Scenariusz 1: Wysoka Precision, Niski Recall<br>\n",
    "Model bardzo ostrożnie oznacza e-mail jako spam.<br>\n",
    "Oznacza jako spam tylko wtedy, gdy jest pewien.<br>\n",
    "Konsekwencja: Przegapi wiele spamów, ale rzadko myli zwykłe e-maile ze spamem.\n",
    "\n",
    "-> Scenariusz 2: Wysoki Recall, Niska Precision<br>\n",
    "Model oznacza wszystkie podejrzane e-maile jako spam.<br>\n",
    "Konsekwencja: Wykrywa prawie cały spam, ale często oznacza zwykłe e-maile jako spam.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**F1-score** to średnia harmoniczna precyzji i recall, która daje dobry balans pomiędzy tymi dwiema miarami. F1-score jest szczególnie użyteczny, gdy dane są niezbalansowane (np. kiedy jedna klasa występuje znacznie częściej niż druga).\n",
    "\n",
    "F1-score używany jest, gdy precision i recall są równie ważne, ale jeśli chcesz podkreślić znaczenie jednej z tych metryk, używasz F_beta-score.\n",
    "\n",
    "-> beta = 1 <- klasyczny F1-score (równoważy precision i recall)<br>\n",
    "-> beta > 1 <- większy nacisk na recall<br>\n",
    "-> beta < 1 <- większy nacisk na precision<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_beta_score(correct, predicted, beta = 1):\n",
    "    prec = precision(correct, predicted)\n",
    "    rec = recall(correct, predicted)\n",
    "    return (1 + beta**2) * (prec * rec) / (beta**2 * prec + rec) if (beta**2 * prec + rec) != 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self, num_features, epochs = 500, learning_rate=0.01, epsilon=1e-10):\n",
    "        self.weights = np.random.rand(num_features)\n",
    "        self.bias = np.random.rand(1)[0]\n",
    "        self.epochs = epochs\n",
    "        self.epsilon = epsilon\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def predict(self, x): # funkcja aktywacji\n",
    "        return np.where(np.dot(x, self.weights) + self.bias >= 0, 1, 0)\n",
    "    \n",
    "    def train(self, x_train, y_train):\n",
    "        for epoch in range(self.epochs):\n",
    "            prev_weights = self.weights.copy()\n",
    "\n",
    "            for x, y in zip(x_train, y_train):\n",
    "                prediction = self.predict(x)\n",
    "                update = y - prediction  # -1, 0, lub 1; jeśli 0 to wagi się nie aktualizują, ponieważ była poprawna predykcja\n",
    "                \n",
    "                self.weights += self.learning_rate * update * x\n",
    "                self.bias += self.learning_rate * update\n",
    "\n",
    "            # Sprawdzenie, czy wagi się stabilizują\n",
    "            if np.linalg.norm(self.weights - prev_weights) < self.epsilon:\n",
    "                return epoch + 1\n",
    "        return self.epochs\n",
    "\n",
    "    def test(self, X):\n",
    "        return self.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[15.73094725  7.18528284 64.41038317 24.66428237]\n",
      " [83.44155279 63.01982687 75.83614336 67.61127283]\n",
      " [99.39100517  9.61315971 14.93347999 72.15498409]\n",
      " ...\n",
      " [87.97905359 25.80428991 22.24284486 88.04449276]\n",
      " [80.20238287 21.64052602 41.48732363  3.6147353 ]\n",
      " [ 0.62492591 99.65803104 81.97150773 77.52295713]]\n",
      "[0 1 1 ... 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "import csv    \n",
    "def open_csv(file_path):\n",
    "    x, y = [], []\n",
    "    with open(file_path, 'r') as csvfile:\n",
    "        csvreader = csv.reader(csvfile, delimiter=' ')\n",
    "        for row in csvreader:\n",
    "            x.append(list(map(float, row[:-1])))\n",
    "            y.append(int(row[-1]))\n",
    "    return np.array(x), np.array(y)\n",
    "\n",
    "x, y = open_csv('dataset.csv')       \n",
    "print(x)\n",
    "print(y)\n",
    "x_train = x[:7000]\n",
    "y_train = y[:7000]\n",
    "\n",
    "x_test = x[7000:]\n",
    "y_test = y[7000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trening zakończony po 500 epokach.\n",
      "[1 1 1 ... 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "perceptron = Perceptron(4)\n",
    "epochs_used = perceptron.train(x_train, y_train)\n",
    "\n",
    "print(f\"Trening zakończony po {epochs_used} epokach.\")\n",
    "\n",
    "predicted = perceptron.test(x_test)\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8676666666666667\n",
      "Precision: 0.7660332541567696\n",
      "Recall: 0.9976798143851509\n",
      "F1 Score: 0.86664427275781\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {accuracy(y_test, predicted)}\")\n",
    "print(f\"Precision: {precision(y_test, predicted)}\")\n",
    "print(f\"Recall: {recall(y_test, predicted)}\")\n",
    "print(f\"F1 Score: {f_beta_score(y_test, predicted)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
